# Ollama Quick Start Configuration
# Minimal setup for local model evaluation

# Primary Model Configuration
primary_model:
  adapter: ollama
  model: llama3
  temperature: 0.7
  max_tokens: 1000
  json_schema: "schemas/basic_response.json"

adapters:
  default_adapter: ollama

  adapters:
    ollama:
      type: ollama
      base_url: http://localhost:11434
      default_model: llama3
      pull_missing_models: true

templates:
  system_prompt: "You are a helpful AI assistant."
  user_prompt: "{{question}}"

schema:
  question:
    type: categorical
    values:
      - "What is the capital of France?"
      - "How does photosynthesis work?"
      - "Explain quantum computing in simple terms."
      - "What are the benefits of renewable energy?"

sampling:
  method: latin_hypercube
  sample_size: 10

oracles:
  primary:
    type: llm_judge
    adapter: ollama
    model: llama3
    system_prompt: "Rate the response quality from 1-5 where 5 is excellent."
    prompt: "Question: {{question}}\nResponse: {{response}}\nRating (1-5):"

analysis:
  confidence_level: 0.95

output:
  format: json

# Advanced evaluation configuration demonstrating includes and inheritance
inherits: base_config.yaml

prompt_id: ${PROMPT_ID:advanced_qa_evaluation}

# Override primary model to use detailed analysis schema
primary_model:
  json_schema: "schemas/detailed_analysis.json"
prompt_template: |
  Context: ${CONTEXT:You are an AI assistant helping with technical questions.}

  Question Type: {{question_type}}
  Difficulty: {{difficulty}}
  Domain: {{domain}}

  Question: {{question}}

  Please provide a {{response_style}} response.

axes:
  question_type:
    type: categorical
    values: ["factual", "analytical", "creative", "problem-solving"]
    weights: [0.3, 0.3, 0.2, 0.2]

  difficulty:
    type: categorical
    values: ["beginner", "intermediate", "advanced"]
    weights: [0.4, 0.4, 0.2]

  domain:
    type: categorical
    values: ["technology", "science", "business", "general"]

  response_style:
    type: categorical
    values: ["concise", "detailed", "step-by-step"]

  question:
    type: categorical
    values: [
      "What is machine learning?",
      "How do neural networks work?",
      "Explain the difference between AI and ML",
      "What are the ethical implications of AI?",
      "How can businesses implement AI solutions?"
    ]

# Oracle configurations for comprehensive evaluation
oracles:
  accuracy:
    type: embedding_similarity
    canonical_answer: |
      A high-quality response should be accurate, comprehensive, and well-structured.
      It should directly address the question or instruction while providing
      appropriate detail for the intended audience.
    method: cosine_similarity
    threshold: 0.85
    embedding_model: text-embedding-3-small

  explainability:
    type: llm_judge
    rubric: |
      Rate the response quality on a scale of 1-5 based on these criteria:
      1. Accuracy and factual correctness
      2. Clarity and coherence of explanation
      3. Appropriate level of detail
      4. Logical structure and flow
      5. Relevance to the specific question

      Score 4-5: Excellent response meeting all criteria
      Score 3: Good response meeting most criteria
      Score 1-2: Poor response with significant issues
    judge_model: gpt-4
    temperature: 0.1
    output_format: numeric

  confidence_calibration:
    type: statistical_calibration
    expected_confidence: 0.8
    tolerance: 0.1
    min_samples: 50

# Override some sampling settings
sampling:
  method: latin_hypercube
  optimization_criterion: maximin
  random_seed: ${RANDOM_SEED:42}
  stratified_by: ["question_type", "difficulty"]

n_variants: ${N_VARIANTS:2000}

domain_context:
  industry: technology
  use_case: advanced_qa_evaluation
  data_sensitivity: internal
  compliance_requirements: ["gdpr", "ccpa"]

# Override metadata
metadata:
  version: 2.0.0
  description: |
    Advanced Q&A evaluation configuration with multiple question types,
    difficulty levels, and comprehensive oracle assessment.
  tags: ["advanced", "qa", "multi-domain", "evaluation"]
  parameters:
    total_combinations: "4 * 3 * 4 * 3 * 5 = 720"
    recommended_variants: "7200+ for statistical significance"
